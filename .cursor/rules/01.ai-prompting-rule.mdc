---
description: Used when you try to prompt or contact any AI service, including but not limited to OpenAI, Anthropic, DeepSeek
globs: 
alwaysApply: false
---
# Prompting

## File Storage

- prompts must be kept in the `prompts` folder and use the `use server` directive
- write the prompts as txt file and put it in the `prompts` folder at root, use `app/services/load-prompts.ts` to load the prompt files 

## Server-Side Prompt Handling

- Files with `'use server'` directive can ONLY export async functions, never objects or constants
- Always create server-side functions that load prompts and return complete configurations
- Example pattern:
```js
// CORRECT - server-side function that loads prompts
'use server';
export async function createAgentConfigs(): Promise<AgentConfig[]> {
  const prompt = await loadPromptFile('agent-prompt.txt');
  return [{ name: 'agent', instructions: prompt }];
}

// INCORRECT - exporting objects from 'use server' file
'use server';
export const agentConfigs = [{ name: 'agent', instructions: 'prompt' }];
```

## Client-Server Separation

- **NEVER expose system prompts to the client** - this is a security vulnerability
- Create separate `.client.ts` files for client-side metadata (names, descriptions)
- Server-side files handle actual prompt loading and sensitive configurations
- Use API endpoints to get configured sessions without exposing prompts

Example architecture:
- `agentConfigs.client.ts` - exports metadata only (names, descriptions)
- `agentConfigs.ts` - server-side with `'use server'`, loads prompts from files
- API endpoint calls server function and returns session config (not raw prompts)

## Model choices

- When asked to prompt, always look online for the latest model information, do NOT use the model in your memory
- For OpenAI, use `gpt-4.1` for normal tasks, and `o3-mini` for chain-of-thoughts tasks
- **IMPORTANT**: If a user has manually changed the model number in code, do NOT attempt to switch it back to what you think is appropriate - respect their choice

## OpenAI API Usage

### Response API vs Completions API (IMPORTANT)
- **ALWAYS use the Response API (`openai.responses.create()`) instead of the Chat Completions API (`openai.chat.completions.create()`)**
- The Response API is OpenAI's modern, stateful API designed for complex workflows, multi-turn conversations, and tool usage
- **CRITICAL**: Since the Response API is relatively new, always search the internet for the latest documentation and implementation examples before implementing
- The Response API has different input/output structures compared to Chat Completions API

### Response API Implementation Guidelines
- **Always search online first** for current Response API documentation and examples
- Use proper input structure with `type: 'message'` for conversation items
- Content should be arrays with typed items (`input_text`, `input_image`, etc.)
- Handle response status checking (`response.status === 'completed'`)
- Extract output from `response.output` array structure

```js
// CORRECT - Modern Response API approach
const response = await openai.responses.create({
  model: 'gpt-4.1',
  input: [
    {
      type: 'message',
      role: 'system',
      content: [
        {
          type: 'input_text',
          text: systemPrompt
        }
      ]
    },
    {
      type: 'message',
      role: 'user',
      content: [
        {
          type: 'input_text',
          text: userMessage
        }
      ]
    }
  ],
  text: {
    format: {
      type: 'json_schema',
      name: 'response_schema',
      schema: yourJsonSchema,
      strict: true
    }
  }
});

// Check response status
if (response.status !== 'completed') {
  throw new Error(`Response not completed. Status: ${response.status}`);
}

// Extract content from Response API output structure
const outputMessage = response.output.find(item => item.type === 'message');
const textContent = outputMessage.content.find(item => item.type === 'output_text');

// INCORRECT - Deprecated Chat Completions API
const response = await openai.chat.completions.create({
  model: 'gpt-4.1',
  messages: [...], // Old format
  response_format: { ... } // Old format
});
```

### Structured Outputs with Response API
- Use `text.format.json_schema` instead of `response_format` for structured outputs
- The schema structure is the same, but the container format is different
- Always use `strict: true` for guaranteed response format
- Handle the Response API's output structure when parsing results

```js
// CORRECT - Response API structured output
text: {
  format: {
    type: 'json_schema',
    name: 'analysis_result',
    schema: yourJsonSchema,
    strict: true
  }
}

// INCORRECT - Chat Completions format (deprecated)
response_format: {
  type: 'json_schema',
  json_schema: {
    name: 'analysis_result',
    schema: yourJsonSchema,
    strict: true
  }
}
```

### Multi-modal Support with Response API
- Use proper content arrays with typed items for images and text
- Include required properties like `detail` for images
- The Response API has native multi-modal support

```js
// CORRECT - Multi-modal input with Response API
{
  type: 'message',
  role: 'user',
  content: [
    { type: 'input_text', text: 'Analyze this image' },
    {
      type: 'input_image',
      image_url: 'data:image/jpeg;base64,${base64Image}',
      detail: 'high'
    }
  ]
}
```

### Schema Definition Best Practices
- Create shared TypeScript interfaces and JSON schemas in the `types` directory
- Use `additionalProperties: false` for strict validation
- Include detailed descriptions for all schema properties
- Reference shared schemas from [types/revision.ts](mdc:server/types/revision.ts) as an example

## OpenAI SDK

- when you create an OpenAI object, make sure to wrap it inside a function. Such that when we build the docker image, it won't fail because the API key is not available
- use `/app/services/openai.ts` for creating OpenAI instance, don't create it repeatedly in every files lol Sometimes we may need to configure it to use other OAI style APIs, which can be quite a pain if we have a new instance on every page

## Error handling

You can assume the `loadPromptFile` will never fail, do not attempt to create a try-catch for it.

```js
// INCORRECT
let systemPrompt;
try {
    systemPrompt = await loadPromptFile('youtube-song-bing-fallback.txt');
} catch (error) {
    console.log('Fallback prompt file not found, using default prompt:', error);
    systemPrompt = `You are a music identification expert...`;
}

// CORRECT
const systemPrompt = await loadPromptFile('youtube-song-bing-fallback.txt');
const response = await openai.responses.create({
  model: 'gpt-4.1',
  input: [
    {
      type: 'message',
      role: 'system',
      content: [{ type: 'input_text', text: systemPrompt }]
    },
    {
      type: 'message',
      role: 'user',
      content: [{ type: 'input_text', text: `Analyze this YouTube video title: "${videoTitle}"` }]
    }
  ],
  text: {
    format: {
      type: 'json_schema',
      name: 'song_analysis',
      schema: songAnalysisSchema,
      strict: true
    }
  }
});
```

## Research Requirement

- **ALWAYS search the internet for the latest OpenAI Response API documentation** before implementing any OpenAI API calls
- The Response API is relatively new and implementation details may have changed since the AI model's training data
- Look for official OpenAI documentation, community examples, and recent implementation patterns
- Pay special attention to input/output structures, error handling, and supported features
